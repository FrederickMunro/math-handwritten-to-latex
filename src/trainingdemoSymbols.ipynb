{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model.cnn import CNN\n",
    "from model.encoder import Encoder\n",
    "from model.decoder import Decoder\n",
    "from model.endtoend import HME2LaTeX\n",
    "from data_processing.loadData import HMEDataset\n",
    "from model.language import Lang, tensorFromSentence, indexesFromSentence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_file = './data/symbol_train_labels.txt'\n",
    "images_directory = './data/symbol_train_png/'\n",
    "\n",
    "dataset = HMEDataset(labels_file, images_directory)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Runs on GPU if cuda is installed, else on CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tuple from index 0\n",
    "image_tensor, target_label, index = dataset[0]\n",
    "\n",
    "# Prints visual of extracted information\n",
    "print(image_tensor)\n",
    "print('-'*70)\n",
    "print(target_label)\n",
    "print('-'*70)\n",
    "print(index)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lang object\n",
    "latex = Lang('latex')\n",
    "\n",
    "# Extract label column from dataset\n",
    "label_list = dataset.labels_file.iloc[:,1]\n",
    "\n",
    "# Populate latex language by cycling through label column\n",
    "for label in label_list:\n",
    "    latex.addSentence(label)\n",
    "\n",
    "# Language display\n",
    "print(latex.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a tensor with each image's label as its index to the latex language\n",
    "labels_latex_index = [tensorFromSentence(latex, i) for i in dataset.labels_file.iloc[:,1]]\n",
    "labels_by_lang_index = torch.cat(labels_latex_index).unsqueeze(1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(device).to(device)\n",
    "encoder = Encoder(input_size=512, hidden_size=256, seq_size=(BATCH_SIZE*31), batch_size=BATCH_SIZE).to(device)\n",
    "decoder = Decoder(input_size=1, hidden_size=512, output_size=latex.n_words, num_features=32*31, batch_size=BATCH_SIZE, device=device).to(device)\n",
    "model  = HME2LaTeX(cnn, encoder, decoder, labels_by_lang_index.shape[0], BATCH_SIZE, latex.n_words, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './symbol_model_4.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment if you want to train existing model:\n",
    "# checkpoint = torch.load(PATH, map_location=device)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# loss_list = checkpoint['loss']\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "accuracy_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "total_batches = len(dataset) // BATCH_SIZE\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch', str(epoch+1) + '/' + str(epochs))\n",
    "    print('\\t' + 'Batch'.ljust(20) + '\\t' + 'Accuracy'.ljust(20) + '\\t' + 'Loss'.ljust(20))\n",
    "\n",
    "    # Iterate through every batch in dataset\n",
    "    for i, (batch_images, batch_labels, batch_indices) in enumerate(train_dataloader):\n",
    "        \n",
    "        # Initialize optimizer gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Load data into device\n",
    "        batch_label_indices = labels_by_lang_index[0][batch_indices].float().to(device)\n",
    "        batch_images = batch_images.float().to(device)\n",
    "\n",
    "        # Forward pass through model\n",
    "        batch_prediction_probabilities = model(batch_images, batch_label_indices)[0]\n",
    "\n",
    "        # Calculate batch accuracy\n",
    "        batch_predicted_labels = torch.argmax(batch_prediction_probabilities, dim=1)\n",
    "        batch_label_indices = batch_label_indices.squeeze(1).long()\n",
    "        correct = torch.sum(batch_predicted_labels == batch_label_indices).item()\n",
    "        batch_accuracy = correct / len(batch_label_indices)\n",
    "        batch_accuracy_percentage = batch_accuracy * 100\n",
    "        \n",
    "\n",
    "        # Calculate batch loss using Categorical Cross Entropy\n",
    "        batch_loss = loss(batch_prediction_probabilities, batch_label_indices)\n",
    "\n",
    "        # Add items to the list for history tracking\n",
    "        loss_list.append(batch_loss)\n",
    "        accuracy_list.append(batch_accuracy_percentage)\n",
    "\n",
    "        # Perform backward propagation to compute gradients\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # Update model parameters using optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log batch information\n",
    "        torch.save({\n",
    "            'model_state_dict' : model.state_dict(),\n",
    "            'optimizer_state_dict' : optimizer.state_dict(),\n",
    "            'loss': batch_loss.item(),\n",
    "            'losses': loss_list,\n",
    "            'accuracies': accuracy_list\n",
    "        }, PATH)\n",
    "        \n",
    "        # Display terminal updates for every 20th batch\n",
    "        if i % 50 == 0 and i != 0:\n",
    "            print('\\t' + (str(i) + '/' + str(total_batches)).ljust(20),\n",
    "                  '\\t' + (str(batch_accuracy_percentage) + '%').ljust(20),\n",
    "                  '\\t' + str(batch_loss.item()).ljust(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b50a1d762d4853fe27a86530a9bcdf85f59f22d585aca88e4f703f55ce46be5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
